{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.120.27.99:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fb39c6351d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- rid: long (nullable = false)\n",
      " |-- msno: string (nullable = true)\n",
      " |-- pay_method: string (nullable = true)\n",
      " |-- plan_days: integer (nullable = true)\n",
      " |-- list_price: string (nullable = true)\n",
      " |-- actual_paid: string (nullable = true)\n",
      " |-- auto_renew: boolean (nullable = true)\n",
      " |-- is_cancel: boolean (nullable = true)\n",
      " |-- trans_date: date (nullable = true)\n",
      " |-- expire_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date, expr, monotonically_increasing_id\n",
    "\n",
    "infile = 'file:///home/cloudera/2.kkbox_churn/raw_data/transactions.csv'\n",
    "df0 = spark.read.format('csv').option('header','true').load(infile) \\\n",
    "  .select(monotonically_increasing_id().alias('rid'),\n",
    "          'msno', \n",
    "          expr('payment_method_id AS pay_method'),\n",
    "          expr('CAST(payment_plan_days AS int) AS plan_days'),\n",
    "          expr('plan_list_price AS list_price'),\n",
    "          expr('actual_amount_paid AS actual_paid'),\n",
    "          expr('CAST(is_auto_renew AS boolean) AS auto_renew'),\n",
    "          expr('CAST(is_cancel AS boolean) AS is_cancel'),\n",
    "          to_date('transaction_date', 'yyyyMMdd').alias('trans_date'),\n",
    "          to_date('membership_expire_date', 'yyyyMMdd').alias('expire_date')\n",
    "         )\n",
    "\n",
    "df0.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+----------+---------+----------+-----------+----------+---------+----------+-----------+\n",
      "|rid|                msno|pay_method|plan_days|list_price|actual_paid|auto_renew|is_cancel|trans_date|expire_date|\n",
      "+---+--------------------+----------+---------+----------+-----------+----------+---------+----------+-----------+\n",
      "| 44|FT4moGxOj6tzwkTSA...|        34|       30|       149|        149|      true|    false|2015-11-30| 2015-12-31|\n",
      "| 45|z1s1E/gm6xiwjNb8T...|        34|       30|       149|        149|      true|    false|2015-11-30| 2015-12-31|\n",
      "| 46|lZyYiuAJW3qzDnicN...|        34|       30|       149|        149|      true|    false|2015-11-30| 2015-12-31|\n",
      "| 47|pXpFcJbT8/FDkhnSU...|        34|       30|       149|        149|      true|    false|2015-11-30| 2015-12-31|\n",
      "| 48|0f9IUy6wP6pEUntXg...|        31|       30|       149|        149|      true|    false|2015-11-30| 2015-12-31|\n",
      "+---+--------------------+----------+---------+----------+-----------+----------+---------+----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter: trans_date >= 2015-11-01\n",
    "from pyspark.sql.functions import col, lit\n",
    "df1 = df0.where(col('trans_date') >= to_date(lit('2015-11-01')))\n",
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dates Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[44, datetime.date(2015, 12, 2), datetime.date(2015, 12, 31)],\n",
       " [45, datetime.date(2015, 12, 2), datetime.date(2015, 12, 31)],\n",
       " [46, datetime.date(2015, 12, 2), datetime.date(2015, 12, 31)]]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime as dt\n",
    "\n",
    "def date_minus_days(d, n):\n",
    "    '''\n",
    "    `d`: an object of class `datetime.date`\n",
    "    `n`: an integer\n",
    "    Returns a `datetime.date` object.\n",
    "    '''\n",
    "    return (dt.datetime.combine(d, dt.time()) - dt.timedelta(days=n)).date()\n",
    "    \n",
    "\n",
    "def dates_correction(is_cancel, trans_date, expire_date, plan_days):\n",
    "    '''\n",
    "    `is_cancel`: boolean\n",
    "    `trans_date`, `expire_date`: datetime.date objects\n",
    "    `plan_days`: integer\n",
    "    Returns [corrected-start-date, corrected-expiration-date]\n",
    "    '''\n",
    "    d_start = None\n",
    "    d_exp = None\n",
    "    \n",
    "    if is_cancel:\n",
    "        d_exp = expire_date if trans_date <= expire_date else trans_date\n",
    "    else: # not is_cancel\n",
    "        start_date = date_minus_days(expire_date, plan_days-1)\n",
    "        if trans_date <= start_date:\n",
    "            d_start = start_date\n",
    "            d_exp = expire_date\n",
    "        elif trans_date <= expire_date:\n",
    "            d_start = trans_date\n",
    "            d_exp = expire_date\n",
    "        else:\n",
    "            d_start = None\n",
    "            d_exp = None\n",
    "            \n",
    "    return [d_start, d_exp]\n",
    "\n",
    "\n",
    "rdd_corr1 = df1.rdd.map(lambda row: [row['rid']] + dates_correction(row['is_cancel'], row['trans_date'], \n",
    "                                                                   row['expire_date'], row['plan_days']))\n",
    "rdd_corr1.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(corr_exp_date=datetime.date(2015, 12, 31), corr_start_date=datetime.date(2015, 12, 2), rid=44),\n",
       " Row(corr_exp_date=datetime.date(2015, 12, 31), corr_start_date=datetime.date(2015, 12, 2), rid=45),\n",
       " Row(corr_exp_date=datetime.date(2015, 12, 31), corr_start_date=datetime.date(2015, 12, 2), rid=46)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "rdd_corr2 = df_corr1.map(lambda rec: Row(rid=rec[0], corr_start_date=rec[1], corr_exp_date=rec[2]))\n",
    "rdd_corr2.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+---+\n",
      "|corr_exp_date|corr_start_date|rid|\n",
      "+-------------+---------------+---+\n",
      "|   2015-12-31|     2015-12-02| 44|\n",
      "|   2015-12-31|     2015-12-02| 45|\n",
      "|   2015-12-31|     2015-12-02| 46|\n",
      "+-------------+---------------+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_corr = rdd_corr2.toDF()\n",
    "df_corr.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Tables\n",
    "Merge `df1` with `df_corr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_corr_ = df_corr.withColumnRenamed('rid', 'rid_')\n",
    "\n",
    "df_final = df1.join(df_corr_, df1['rid']==df_corr_['rid_'], 'inner') \\\n",
    "  .drop('rid').drop('rid_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- msno: string (nullable = true)\n",
      " |-- pay_method: string (nullable = true)\n",
      " |-- plan_days: integer (nullable = true)\n",
      " |-- list_price: string (nullable = true)\n",
      " |-- actual_paid: string (nullable = true)\n",
      " |-- auto_renew: boolean (nullable = true)\n",
      " |-- is_cancel: boolean (nullable = true)\n",
      " |-- trans_date: date (nullable = true)\n",
      " |-- expire_date: date (nullable = true)\n",
      " |-- corr_exp_date: date (nullable = true)\n",
      " |-- corr_start_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# output\n",
    "outfile = 'file:///home/cloudera/2.kkbox_churn/data01/from_raw_transactions-v1/with_corrected_dates'\n",
    "df_final.write.format('csv').option('header','true').save(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
